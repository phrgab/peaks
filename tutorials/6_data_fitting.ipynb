{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Fitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import peaks as pks\n",
    "import os\n",
    "\n",
    "# Set default options\n",
    "xr.set_options(cmap_sequential='Purples', keep_attrs=True)\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "# Example data\n",
    "from peaks.core.utils.sample_data import ExampleData\n",
    "\n",
    "# For rendering output from iplot\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "This tutorial gives a brief introduction to fitting data within `peaks`. `peaks` utilises :class:`lmfit` for fitting data, with some added or adapted methods to ease using this with data in a :class:`xarray.Dataarray` format, and some additional helper functions. The user should be familiar with :class:`lmfit` Models and methodoligies, see the `lmfit` [documentation](https://lmfit.github.io/lmfit-py/index.html).\n",
    "\n",
    ":::{tip}\n",
    "`peaks` wraps all standard :class:`lmfit.models` and provides some specific additional ones. These can be accessed at `peaks.core.fitting.models`. Any other compatible :class:`lmfit.model.Model` can be configured for use with `peaks` as\n",
    "\n",
    "```python\n",
    "from peaks.core.fitting.models import create_xarray_compatible_lmfit_model\n",
    "\n",
    "# Make a compatible version of some lmfit-type `OriginalModel`\n",
    "NewModel = create_xarray_compatible_lmfit_model(OriginalModel)\n",
    "```\n",
    ":::\n",
    "\n",
    ":::{warning}\n",
    "At present, `peaks` does not support fitting with units. The relevant :class:`xarray.DataArray`s are *dequantified* before fitting, and fit parameters should be specified without units.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## General peak fitting\n",
    "### Fitting 1D data\n",
    "Let's generate some 1D data to fit, here a single MDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dispersion\n",
    "disp = ExampleData.dispersion()\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract an MDC for initial analysis\n",
    "cut1 = disp.MDC(105.06, 0.005).sel(theta_par=slice(0,15))#.pint.dequantify()\n",
    "cut1.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "We'll fit this with the following model:\n",
    "\n",
    "- 3 Lorentzian peaks\n",
    "- Linear background\n",
    "- Convolve with a Gaussian for experimental angular resolution\n",
    "\n",
    "First we import the relevant base peak models and a model for performing the Gaussian convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peaks.core.fitting.models import LorentzianModel, LinearModel, GaussianConvolvedFitModel, ConstantModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Now we build the model and initialise the parameters. The initial model is built using the `peaks`-wrapped versions of the standard models combined with binary operators (forming a :class:`lmfit.model.CompositeModel`), and then this is passed to the :function:`peaks.core.fitting.models.GaussianConvolvedFitModel` to add the Gaussian convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "base_model = LinearModel(prefix=\"bg_\")\n",
    "for i in range(3):\n",
    "    base_model += LorentzianModel(prefix=f\"p{i}_\")\n",
    "model = GaussianConvolvedFitModel(base_model)\n",
    "\n",
    "# Make the fit parameter list\n",
    "params = model.make_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all the parameters\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "We can manually enter sensible starting parameters, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.make_params(sigma_conv=dict(value=0.1, vary=False))  # e.g. fix the resolution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Or we can make use of the estimating capabilities of the individual models. This is not implemeted for all models, but is for most. It also does not work for the complete composite model in one go, and so we iterate through each peak and component in turn..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate theta_par ranges for the peaks\n",
    "p_range = {}\n",
    "p_range[0] = [4,7.5]\n",
    "p_range[1] = [7.5,9]\n",
    "p_range[2] = [9,11]\n",
    "pks.plot_grid([cut1.sel(theta_par=slice(p_range[i][0],p_range[i][1])) for i in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate fit parameters over these ranges\n",
    "for i in range(3):\n",
    "    t1,t2 = p_range[i]\n",
    "    DC_data = cut1.sel(theta_par=slice(t1,t2))  # Select data over this range\n",
    "    # With the `peaks` wrapped fit models, we pass the 1D DataArray directly instead of seperate y and x values\n",
    "    peak_param_guess = LorentzianModel(prefix=f\"p{i}_\").guess(DC_data)  # Guess the initial params\n",
    "    # Iterate through the guessed params and update the main params dictionary\n",
    "    params.update(peak_param_guess)\n",
    "\n",
    "# Make a guess for the background from the start of the range\n",
    "DC_data = cut1.sel(theta_par=slice(0,4))\n",
    "bg_param_guess = LinearModel(prefix=\"bg_\").guess(DC_data)\n",
    "params.update(bg_param_guess)\n",
    "\n",
    "# Show updated params\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "We can plot the calcuated model using our guessed parameters on top of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut1.plot_fit_test(model=model, params=params, show_components=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "This looks a reasonable starting point, so we can now fit the model. For this, we can make use of the :class:`peaks.core.fitting.fit` method, passing the data as an :class:`xarray.DataArray`. As this is a 1D DataArray, we do not need to explicitly specify the independent dimension (but we would if we had passed a higher dimensional array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_result = cut1.fit(model, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "This returns a :class:`xarray.DataSet` which contains the results of the fit and standard errors as variables, as well as the full :class:`lmfit.model.ModelResult`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full lmfit ModelResult\n",
    "fit_result['fit_model'].values[()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "#### Plotting fits\n",
    "The fit, components and residual can be plot using the method `.plot_fit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_result.plot_fit(figsize=(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "#### Saving and loading fits\n",
    "We can save the fit results (including the full model) and recover them using the `.save_fit` and `.load_fit` methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_result.save_fit('fit_example')\n",
    "previous_fit_result = pks.load_fit('fit_example')\n",
    "if previous_fit_result == fit_result:\n",
    "    print(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Fitting multi-dimensional data\n",
    "We'll now fit a series of MDCs from this dataset. We can either simply select the data over the required range we wish to fit, or for more complex fit regions, can mask the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp.plot()\n",
    "# Define a custom mask region\n",
    "mask_region = {'eV': [105.06, 105.04, 105.04, 105.06],\n",
    "              'theta_par': [4, 6, 13, 13]}\n",
    "# Note - choosing this region for illustration of the method, not because it is a particularly sensible region to fit over...!\n",
    "\n",
    "# Plot the mask\n",
    "pks.plot_ROI(mask_region, y='eV', linestyle='--')\n",
    "\n",
    "# Select the data\n",
    "data_to_fit = disp.mask_data(mask_region, return_integrated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_fit.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "As we have now masked the data in a way that leaves NaNs at the edges for some of the data sets, we need to ensure the fit model can cope with this. To do this, we set the model `nan_policy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.nan_policy = 'omit'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "For a 2D DataArray like this, we can choose to fit the data sequentially, where the result from the previous fit is used to initiate the starting parameters of the next fit, or using the same starting parameters for all fits (the latter also allows fitting the data using parallel processing). We will use the sequential mode here, which is the default when we are fitting 2D data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the data - note we have reversed the direction in which to perform the sequential fit as we \n",
    "# figured out our initial guess parameters for the `last` MDC in this DataArray\n",
    "%time fit_result2 = data_to_fit.fit(model, params, independent_var='theta_par', reverse_sequential_fit_order = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "We can now plot the relevant results of the fit directly from the returned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5), ncols=2)\n",
    "data_to_fit.plot(ax=ax[0])\n",
    "fit_result2['p0_center'].plot(ax=ax[0], y='eV', marker='o')\n",
    "fit_result2['p1_center'].plot(ax=ax[0], y='eV', marker='o')\n",
    "fit_result2['p2_center'].plot(ax=ax[0], y='eV', marker='o')\n",
    "\n",
    "# Plot the fit with the error bars\n",
    "ax[1].errorbar(x=fit_result2['p0_center'].data,\n",
    "           y=fit_result2['p0_center'].eV.data,\n",
    "           yerr=fit_result2['p0_center_stderr'].data,\n",
    "        marker='o')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "Calling the `plot_fit()` function on the fit result will now allow us to dynamically explore the fits and their residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_result2.plot_fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### Dask arrays\n",
    "The above methodology extends naturally to higher-dimensional data sets, although then sequential fitting cannot be used. If fitting higher-dimensional data (and sometimes also for lower-dimensional data), it is worth considering using :class:`dask`-backed :class:`xarray.DataArray`'s. This allows the fit to be performed in parallel, or in a lazy manner so that it is only computed when required. It is worth considering how best to `chunk` the DataArray to optimise for the intended fitting.\n",
    "\n",
    ":::{tip}\n",
    "Even though the `xarray.DataArray`s units are stripped during the fit function call, the `dask` graph still seems to retain some unit information, and so triggering the delayed computation can emit one or many `UnitStrippedWarning`s. There is a utility context manager (used below) to filter these warnings as a temporary workaround until we can solve this problem.\n",
    ":::\n",
    "\n",
    "Let's load a section of a spatial map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "SM1 = (ExampleData.SM()  # File to load\n",
    "       .isel(x2=slice(None,15))  # Select limited spatial range from the map\n",
    "       .sel(theta_par=slice(0,5),eV=slice(74,76))  # Select limited angle and energy range\n",
    "       .chunk(eV=-1,x1=1,x2=1)  # Ensure only a single chunk of the Dask array along the eV axis, but chunked in x1 and x2 \n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "SM1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a single EDC for illustration and setting up the fit\n",
    "EDC_ = SM1.isel(x1=0,x2=0,theta_par=0)\n",
    "EDC_.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model = LorentzianModel() + ConstantModel()\n",
    "params = fit_model.make_params(amplitude=20,\n",
    "                              center=dict(value=75.25, min=74.5, max=75.7),\n",
    "                              sigma=0.1,\n",
    "                              c=dict(value=4, min=0)\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time fit_results3 = SM1.fit(fit_model, params, independent_var='eV')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "No fitting has been performed yet (note that this was extremely quick!), but we can still access the fit results plot, with the fits being performed as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_results3.plot_fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "And we can make plots of relevant fit results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peaks.core.utils.misc import silence_unit_warnings\n",
    "with silence_unit_warnings():\n",
    "    fit_results3['center'].isel(theta_par=10).plot()  # This is the important part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "## Fitting Au Fermi edge data\n",
    "\n",
    "For fitting Au Fermi edge data, we employ a model :class:`peaks.core.fitting.LinearDosFermiModel` which employs a linear DOS, and Gaussian-broadened Fermi function, and a linear background above $E_F$ to account for potential detector non-linearities. This model can be imported and manually fit using the methods above, but we also have a utility method `fit_gold` which attempts an automated fitting to this model, and displays some key results as well as returning the actual fit result :class:`xarray.DataSet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a gold scan and restrict the range to a sensible amount\n",
    "gold_scan = ExampleData.gold_reference4().sel(eV=slice(16.4,17),theta_par=slice(-10,10))\n",
    "gold_scan.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "We can pass a single EDC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_result = gold_scan.isel(theta_par=0).fit_gold()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "The fitted Fermi energy is also stored in the fit_result attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_result.attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "Or we can pass the measured angular-dependent Au reference, optionally specifying the type of function (polynomaial, average etc.) used to determine the Fermi level dependence on detector angle (via the `EF_correction_type` parameter; defaults to polynomail of order 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_result = gold_scan.fit_gold()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "The results of the `EF_correction` fit are stored as a dictionary in the fit results attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_result.EF_correction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "## Quick fitting\n",
    "\n",
    "For simple functions (linear, polynomial of some degree, single peak + linear background), a quite fit can be performed by passing either a 1D DataArray, or a multi-dimensional array and specifying the independent variable. The quick fit is accessed via the `.quick_fit.MODEL` accessor, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_DC = gold_scan.isel(theta_par=0).sel(eV=slice(None, 16.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_result = example_DC.quick_fit.linear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "This returns the usual `peaks` :class:`xarray.DataSet` fit_result. The parameters can be seen directly: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "Or the plot can be returned as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_result.plot_fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "This method scales to higher-dimensional data as for the full fit methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "SM1.quick_fit.lorentzian(independent_var='eV').plot_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
