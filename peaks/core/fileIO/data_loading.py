# Functions to load data from a beamline scan file into an xarray
# Brendan Edwards 28/06/2021
# PK: major re-write to file loading to implement for dask arrays starting 14/5/22

import os
from os.path import isfile, join
import natsort
import h5py
import gc
from tqdm.notebook import tqdm
from tqdm.dask import TqdmCallback
import ase.io
import dask

from peaks.core.fileIO.loaders.SES_loader import SES_txt_load, SES_zip_load, my_find_SES
from peaks.core.fileIO.loaders.igor_pro_to_xarray import read_ibw_Wnote
from peaks.core.process import merge_data
from peaks.utils.sum_spectra import sum_spec
from peaks.utils.metadata import update_hist
from peaks.utils.misc import ana_warn
from peaks.core.fileIO import fileIO_opts


def load(fname, **kwargs):
    '''Shortcut function to load_data with much of the file path set by global options:
        fileIO_opts.fname.path
        fileIO_opts.fname.ext - can be a list of multiple extensions

    Parameters
    ------------
    fname : int or str or list
        if int or str : Remainder (or start) of file name not already specified in global options
        if list : List of the above

    **kwargs : optional
        Additional options that can be passed to `peaks.fileIO.load_data`


    Returns
    ------------
    xr.DataArray or xr.DataSet or list(xr.DataArray) or list(xr.DataSet)
        xarray DataArray or DataSet with loaded data <br>
        Returned with arrays as dask objects or list of these
        where data is generated by load_data(fileIO_opts.fname.path + fname + fileIO_opts.fname.ext)


    Examples
    ------------
    from peaks import *

    # First set file strings for starting and ending part of the file name:
    fileIO_opts.fname.path = '../example_data/ARPES/i05-34'
    fileIO_opts.fname.ext = '.nxs'
    # Can also supply multiple possible extensions as a list, e.g. ['.ibw','.zip']

    # Load single scan
    a = load(237)  # This corresponds to loading scan i05-34237.nxs

    # Load multiple scans
    a = load([237,239])

    # Can be called with other options as specified in `peaks.fileIO.load_data`

    '''

    try:
        fpath = fileIO_opts.fname.path
        ext = fileIO_opts.fname.ext

        if isinstance(ext, list):
            ext = tuple(ext)

        if isinstance(fname, list):
            fname = [str(item) for item in fname]
        else:
            fname = [str(fname)]

        # Split off file path and starting part of file
        if fpath:
            path, start = os.path.split(fpath)
        else:
            path = os.getcwd()
            start = ''

        file_list = natsort.natsorted(os.listdir(path))

        if ext:  # Limit to only allowed extension types
            file_list = [item for item in file_list if item.endswith(ext)]

        if len(start) > 0:  # Limit to only specified file start
            file_list = [item for item in file_list if item.startswith(start)]

        # Limit to files containing specified file identifier
        file_list = [os.path.join(path, item) for item in file_list if any(word in item for word in fname)]

        return (load_data(file_list, **kwargs))
    except:
        raise Exception(
            'File could not be determinted. Please set fileIO_opts.fname.path and fileIO_opts.fname.ext to better define directory/starting string of file and extension, respectively.')


def load_data(fname, loc=None, extension=None, sum_scans=False, merge_scans=False, merge_kwargs=None, sum_sweeps=True,
              binning=None, name=None, chunks='auto', persist=True, parallel=None, Artemis_kw=None, **slice_kwargs):
    '''Load data, returning an xarray with data stored in dask arrays by default.
    
    Parameters
    ----------
    fname : str or list
        if str : Path to the file or folder to be loaded
        if list : List of strings pointing to multiple scans to load

    loc : str, optional
        Name of beamline or other data designator <br>
        If None supplied, will be attempted to be determined from data, but can speed up loading
        for some files if supplied <br>
        For currently supported options, see `fileIO.fileIO_opts.loc_opts`

    extension : str, optional
        Specify file extension (e.g. .txt) <br>
        Useful  if a folder of scans is being loaded.

    sum_scans : bool, optional
        Specifies whether the passed scans (in a folder, or when passed as a list) should be summed on load. <br>
        Defaults to False.

    merge_scans : bool, optional
        Specifies whether the passed scans (in a folder, or when passed as a list) should be merged on load. <br>
        Defaults to False.

    merge_kwargs : dict, optional
        Any required options to pass to merging function.
        see `peaks.process.merging.merge_data`

    sum_sweeps : bool, optional
        Specifies whether scans recorded in 'add dimension' mode should be summed on load. <br>
        Defaults to True.

    binning : int or dict, optional
        Apply binning of data on load <br>
        Pass integer `n` for `n` x `n` binning of the `eV` and `theta_par` dims on load <br>
        Pass dictionary in the format `{dim1: n1, dim2: n2}` to define custom binning by dimension <br>
        Pass `binning=1` to force no binning even for large files.

    name : str, optional
        Specify scan name, otherwise the name will be attempted to be inferred from the file name or metadata <br>
        If a folder or list of scans is to be loaded, must be specified as a list of length len(file_list).

    chunks : str or dict, optional
        Specification of chunk formatting <br>
            'auto' (default) performs auto-chunking <br>
            dict in form {'ax1': chunk_size1, 'ax2': chunk_size2} <br>
            NB chunk size will get rescaled by any binning

    persist : bool, optional
        For a dask array, retains array in memory if True. Useful for faster subsequent processing. <br>
        NB, ensure that scan to be loaded will fit in memory if using.

    parallel : bool, optional
        For a folder of scans to load, will attempt to load in parallel. <br>
        Defaults to True if >10 scans to be loaded, and False otherwise.

    Artemis_kw : dict, optional
        Dictionary of optional arguments to pass to the Artemis file loader
        For data transformation:
        - Ang_offset_px
        - Edge_pos
        - Edge_slope
        For partial data loading:
        - N - number of scans to load

    **slice_kwargs : optional
        Additional keyword arguments to load only a restricted data range. <br>
        Call with dim=slice(a,b) where dim is a dimension name and (a,b) are a range to restrict to on load,
        e.g. eV=slice(16.0,16.9)


    Returns
    ------------
    xr.DataArray or xr.DataSet or list(xr.DataArray) or list(xr.DataSet)
        xarray DataArray or DataSet with loaded data <br>
        Returned with arrays as dask objects


    Examples
    ------------
    ```
    from peaks import *

    # Simplest call, with full data loaded using defaults, and beamline determined automatically:
    a = load_data('../example_data/ARPES/i05-34237.nxs')

    # Can be called with the beamline specified, which can speed up loading for some file formats
    a = load_data('../example_data/ARPES/i05-34237.nxs', loc='Diamond I05-HR')

    # To load a list of files call with a list of filenames. Data can be loaded in a lazy manner by calling with `persist=False`
    a = load_data(['../example_data/ARPES/i05-34237.nxs','../example_data/ARPES/i05-34239.nxs',persist=False)
    ```
    '''

    # if supplied, check beamline designation is from supported list
    if loc and loc not in fileIO_opts.loc_opts:
        loc = None
        warning_str = 'Supplied file identifier (beamline, source) not in supported options. Currently suppoted options are:<br>' + str(
            fileIO_opts.loc_opts) + '<br>Attempting to determine beamline automatically.'
        ana_warn(warning_str)

    # Determine full list of items to load
    if isinstance(fname, list):  # If a list of file names is supplied
        file_list = fname
    else:
        # get scan extension
        filename, file_extension = os.path.splitext(fname)

        # if the input is a folder
        if file_extension == '':
            # get files in the folder
            file_list = natsort.natsorted(os.listdir(fname))

            # Check if folder contains general different scans or is a couple of other special options

            # If a SOLEIL map, folder should contain scans with ROI1 in the filename
            # fname should be passed as single object to single data loader
            file_list_ROI = [item for item in file_list if 'ROI1_' in item and isfile(join(fname, item))]
            file_list_Neq = [item for item in file_list if 'N=' in item]
            if len(file_list_ROI) > 1:
                file_list = [fname]
            # If an ARTEMIS scan collection, folder should contain folders with N= in the foldername
            # fname should be passed as single object to single data loader
            elif len(file_list_Neq) > 0:
                file_list = [fname]
            else:
                # Filter for required extensions to load and filter out other folders etc.
                if extension is not None:
                    file_list = [join(fname, item) for item in file_list if
                                 item.endswith(extension) and isfile(join(fname, item))]
                else:
                    file_list = [join(fname, item) for item in file_list if isfile(join(fname, item))]
        else:  # Just a single file was passed
            file_list = [fname]

    # # If not specified, determine whether to use parallel loading based on numbers of files to load and load data
    # if not parallel:
    #     if len(file_list) > 9:
    #         parallel = True
    #     else:
    #         parallel = False

    # Load data
    data = []

    if parallel:
        # Load all data files with a dask delayed function
        for i, scan in enumerate(file_list):
            data.append(dask.delayed(load_single_data(file=scan, loc=loc, Artemis_kw=Artemis_kw)))

        # Perform the data loading in parallel
        with TqdmCallback(desc="Loading data"):
            data = dask.compute(*data)
    else:
        # Load all data files immediately
        for i, scan in enumerate(tqdm(file_list, desc='Loading data', disable=len(file_list) == 1)):
            data.append(load_single_data(file=scan, loc=loc, Artemis_kw=Artemis_kw))

    nscan = len(data)

    # Apply any range selection if called
    if slice_kwargs:  # If function is called with range selection
        for i in range(nscan):
            data[i] = data[i].sel(slice_kwargs)
            # Add a note to the history about the cropping
            hist = 'Data cropped on load: ' + str(slice_kwargs)
            update_hist(data[i], hist)

    # Overwrite default names if required
    if name:
        if isinstance(name, list):
            for i in range(nscan):
                data[i] = data[i].rename(name[i])
        else:
            for i in range(nscan):
                data[i] = data[i].rename(name)

    # Manual specification of chunk sizes
    if isinstance(chunks, dict):
        for i in range(nscan):
            chunk_temp = chunks

            # Set any unspecified values to single chunk
            for j in data[i].dims:
                if j not in chunk_temp:
                    chunk_temp[j] = data[i][j].size

            # Rechunk the data
            data[i] = data[i].chunk(chunk_temp)

    # Perform binning if required
    for i in range(nscan):
        # Automatically apply 2x2 binning for large files if not specified
        if data[i].size > 5e8 and not binning:
            data[i] = data[i].coarsen({'eV': 2, 'theta_par': 2}, boundary='pad').mean()
            warning_str = 'Large file detected for scan ' + str(file_list[i]) \
                          + '. Automatic 2x2 binning applied. To load without binning, call loading function with option `binning=1`.'
            ana_warn(warning_str)
            # Add a note to the history about the binning
            hist = 'Data binned on load: 2x2 in energy and angle'
            update_hist(data[i], hist)

        # Apply binning defined by an integer
        elif isinstance(binning, int):
            if binning > 1:
                data[i] = data[i].coarsen({'eV': binning, 'theta_par': binning}, boundary='pad').mean()
                # Add a note to the history about the binning
                hist = 'Data binned on load: ' + str(binning) + 'x' + str(binning) + ' in energy and angle'
                update_hist(data[i], hist)

        elif not binning:
            pass

        # Apply custom binning via dict. call
        else:
            data[i] = data[i].coarsen(binning, boundary='pad').mean()

            # Add a note to the history about the binning
            hist = 'Data binned on load: ' + str(binning)
            update_hist(data[i], hist)

    # Sum sweeps on load if required
    if sum_sweeps:
        for i in range(nscan):
            if 'scan_no' in data[i].dims:
                data[i] = data[i].sum('scan_no', keep_attrs=True)
                hist = 'Measured as \'add dimension\'; scans summed on load'
                update_hist(data[i], hist)

    # Sum scans on load if required
    if sum_scans:
        data = sum_spec(*data)

    # Merge scans on load if required
    if merge_scans:
        data = merge_data(data, **merge_kwargs)

    if persist:
        for i in range(nscan):
            if data[i].size > 1e7:
                with TqdmCallback(desc="Persisting data to memory"):  # For larger files, use a progressbar
                    data[i] = data[i].persist()
            else:
                data[i] = data[i].persist()

    # Clear the trash
    gc.collect()
    gc.collect()

    # Return list of loaded files, or if only a single file loaded, return that entry not contained within a list
    return data if nscan > 1 else data[0]


def load_single_data(file, loc, Artemis_kw=None, **kwargs):
    '''This function produces an xarray containing the data of the file being scanned
    
    Input:
        file - Path to the file to be loaded (string)
        loc - BL
        Artemis_kw - optional arguments for Artemis loader
        **kwargs - Additional options options
            Partial loading of data (only works for I05 Nano-ARPES so far):
                - eV = slice(E0,E1) for loading restricted energy range (E0, E1, floats)
                - theta_par = slice(th0,th1) for loading restricted detector angle range (th0, th1, floats)
                - bin_factor=n for n x n binning of the eV and theta_par dims on load (n int)
            sum_sweeps - whether scans should be summed on a multiple scan dispersion - default is True (Boolean)
            Name scan (works for XRD loaders)
                - name = (string), for supplying
    
    Returns:
        data - DataArray or DataSet with loaded data (xarray)'''

    # identify which beamline the scan was obtained at if not already specified
    if loc is None:
        BL = get_beamline(file)
    else:
        BL = loc
    logbook = False

    # load data into xarray
    if BL == 'Diamond I05-HR':
        from .loaders.i05HR_to_xarray import load_i05HR_data
        data = load_i05HR_data(file, logbook)

    elif BL == 'Diamond I05-nano':
        from .loaders.i05nano_to_xarray import load_i05nano_data
        data = load_i05nano_data(file, logbook)

    elif BL == 'ALBA LOREA':
        from .loaders.lorea_to_xarray import load_lorea_data
        data = load_lorea_data(file, logbook)

    elif BL == 'St Andrews - Phoibos':
        from .loaders.StA_phoibos_to_xarray import load_StA_phoibos_data
        data = load_StA_phoibos_data(file, logbook)

    elif BL == 'St Andrews - MBS':
        from .loaders.StA_MBS_to_xarray import load_StA_MBS_data
        data = load_StA_MBS_data(file, logbook)

    elif BL == 'Elettra APE':
        from .loaders.APE_to_xarray import load_APE_data
        data = load_APE_data(file, logbook)

    elif BL == 'MAX IV Bloch':
        from .loaders.bloch_to_xarray import load_bloch_data
        data = load_bloch_data(file, logbook)

    elif BL == 'SOLEIL CASSIOPEE':
        from .loaders.cassiopee_to_xarray import load_cassiopee_data
        data = load_cassiopee_data(file, logbook)

    elif BL == 'Igor Pro file':
        from .loaders.igor_pro_to_xarray import load_igor_pro_data
        data = load_igor_pro_data(file, logbook)

    elif BL == 'netCDF file':
        from .loaders.netCDF_to_xarray import load_netCDF_data
        data = load_netCDF_data(file)

    elif BL == 'Bruker XRD':
        from .loaders.Bruker_to_xarray import load_Bruker_data
        data = load_Bruker_data(file)

    elif BL == 'Artemis':
        from .loaders.Artemis_to_xarray import load_Artemis_data
        if Artemis_kw:
            data = load_Artemis_data(file, **Artemis_kw)
        else:
            data = load_Artemis_data(file)

    else:
        err_str = 'Data source is not supported or could not be identified. Currently suppoted options are:<br>' + str(
            fileIO_opts.loc_opts)
        ana_warn(err_str, warn_type='danger', title='Loading data failed:')
        return None

    # Do a check that all dimensions are ordered low to high
    for i in data.dims:
        if len(data[i]) > 1:
            if data[i].data[1] - data[i].data[0] < 0:  # Array currently has decreasing order
                data = data.reindex({i: data[i][::-1]})

    # Put dimensions into a standard order
    data = data.transpose('t', 'hv', 'temp_sample', 'defocus', 'focus', 'x1', 'x2', 'x3', 'polar', 'ana_polar', 'tilt',
                          'azi', 'defl_perp', 'defl_par', 'da30_z', 'dim0', 'dim1', 'eV', 'theta_par', 'k_par',
                          'y_scale', 'phi', 'two_th', 'scan_no', missing_dims='ignore')

    # Clean up the memory
    gc.collect()

    return data


def get_beamline(file):
    '''This function determines the beamline at which the data within the file was measured
    
    Input:
        file - A string of the path to file to be loaded (string)
    
    Returns:
        BL - The name of the beamline (string)'''

    # get scan extension
    filename, file_extension = os.path.splitext(file)
    BL = 'Undefined'

    # for Soleil Fermi map or ARTEMIS data, the data is in a folder so no extension
    if file_extension == '':
        file_list = natsort.natsorted(os.listdir(filename))
        file_list_ROI = [item for item in file_list if 'ROI1_' in item and isfile(join(file, item))]
        file_list_Neq = [item for item in file_list if 'N=' in item]
        if len(file_list_ROI) > 1:  # SOLEIL
            BL = 'SOLEIL CASSIOPEE'
        elif len(file_list_Neq) > 0:
            BL = 'Artemis'

    elif file_extension == '.xy':
        with open(file) as f:
            line0 = f.readline()  # Read first line of file
        if 'SpecsLab' in line0:
            # St Andrews ARPES system using SpecsLab Prodigy uses xy files
            BL = 'St Andrews - Phoibos'
        elif 'Anode' in line0:  # Bruker XRD also often saved in .xy
            BL = 'Bruker XRD'

    # St Andrews spin-ARPES system is the only one to use krx files
    elif file_extension == '.krx':
        BL = 'St Andrews - MBS'

    # txt files used by St Andrews spin-ARPES system, MAX IV, Elettra and Soleil. Or it could be an Igor Pro txt file
    elif file_extension == '.txt':
        with open(file) as f:
            line = f.readline()  # Read first line from file
        # MAX IV, Elettra and Soleil txt files follow same basic data format so can identify beamline from location line in file
        if line == '[Info]\n':  # SES data format
            meta = SES_txt_load(file, logbook=True)
            location = my_find_SES(meta, 'Location=')
            if 'bloch' in location.lower() or 'maxiv' in location.lower():
                BL = 'MAX IV Bloch'
            elif 'ape' in location.lower() or 'elettra' in location.lower():
                BL = 'Elettra APE'
            elif 'cassiopee' in location.lower() or 'soleil' in location.lower():
                BL = 'SOLEIL CASSIOPEE'
        # Igor Pro txt file
        elif line == 'IGOR\n':
            BL = 'Igor Pro file'
        # St Andrews spin-ARPES system follows a different data format
        else:
            BL = 'St Andrews - MBS'

    # .ibw Igor binary wave files, used as one of the outputs of Scienta SES, so many places
    elif file_extension == '.ibw':
        meta = read_ibw_Wnote(file)
        if 'bloch' in meta.lower() or 'maxiv' in meta.lower():
            BL = 'MAX IV Bloch'
        elif 'ape' in meta.lower() or 'elettra' in meta.lower():
            BL = 'Elettra APE'
        elif 'cassiopee' in meta.lower() or 'soleil' in meta.lower():
            BL = 'SOLEIL CASSIOPEE'
        else:  # Generic ibw
            BL = 'Igor Pro file'

    elif file_extension == '.zip':
        # Only SES .zip files supported so far
        meta = SES_zip_load(file, logbook=True)
        location = my_find_SES(meta, 'Location=')
        if 'bloch' in location.lower() or 'maxiv' in location.lower():
            BL = 'MAX IV Bloch'
        elif 'ape' in location.lower() or 'elettra' in location.lower():
            BL = 'Elettra APE'
        elif 'cassiopee' in location.lower() or 'soleil' in location.lower():
            BL = 'SOLEIL CASSIOPEE'
        elif 'i05' in location or 'diamond' in location:
            BL = 'Diamond I05-nano'

    elif file_extension == '.nxs':
        # Diamond i05's HR and nano branch follow same basic data format, and can be identified by being named i05 or i05-1 respectively
        # LOREA@ALBA also uses essentially the same format
        f = h5py.File(file, 'r')
        nxs_indentifier = h5py_str(f['entry1/instrument/name'])

        if 'i05-1' in nxs_indentifier:
            BL = 'Diamond I05-nano'
        elif 'i05' in nxs_indentifier:
            BL = 'Diamond I05-HR'
        elif 'lorea' in nxs_indentifier:
            BL = 'ALBA LOREA'

    # .nc files are netCDF files
    elif file_extension == '.nc':
        BL = 'netCDF file'

    return BL


def my_find(lines, item):
    '''This function will loop over the lines in the file and then pick out the line
    starting with the desired keyword. 
    
    Input:
        lines - A list, where each entry is a line from the text file (list)
        item - The word or group of characters you are searching for (string)
        
    Returns:
        The line starting with item (string)
    '''

    for line in lines:
        if line.startswith(item) and '=' in line:
            return (line.split('='))[1].strip()
        elif line.startswith(item) and '\t' in line:
            return (line.split('\t'))[1].strip()


def load_structure(file):
    '''Read a .cif file using aso.io.read, and store as an ase.atoms.Atoms object.  

        Input:
            file - Path to the cif file to be loaded (string)
           
        Returns:
            cryst - Crystal structure (Atoms object) '''

    cryst = ase.io.read(file)

    return cryst


# Helper function to return string from h5py format
def h5py_str(f):
    '''
    Parse string or binary string into correct format when reading from h5py file

    Parameters
    ----------
    f : str or bin or list(str) or list(bin)
        file handle of h5py file object

    Returns
    -------

    str
        String of field contents

    '''
    try:
        if not isinstance(f[()], str):
            try:
                return f[0].decode()
            except:
                return f[()].decode()
        else:
            return f[()]
    except:
        return None
