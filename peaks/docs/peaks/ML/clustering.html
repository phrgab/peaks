<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>peaks.ML.clustering API documentation</title>
<meta name="description" content="Unsupervised clustering functions." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>peaks.ML.clustering</code></h1>
</header>
<section id="section-intro">
<p>Unsupervised clustering functions.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Unsupervised clustering functions.

&#34;&#34;&#34;

# Brendan Edwards 31/10/2023

import numpy as np
import xarray as xr
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm
from IPython.display import clear_output
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from peaks.core.utils.OOP_method import add_methods
from peaks.core.utils.misc import analysis_warning
from peaks.core.display.plotting import plot_grid


@add_methods(xr.DataArray)
def xarray_to_ML_format(data, extract=&#39;dispersion&#39;, E=0, dE=0, k=0, dk=0, scale=False, norm=False):
    &#34;&#34;&#34;Represent spatial mapping data as a tabular pandas dataframe where each spatial position is a feature.
    This dataframe format is used to represent data in machine learning functions.

    Parameters
    ------------
    data : xr.DataArray
        The spatial mapping data to change to a tabular pandas dataframe.

    extract : str (optional)
        Determines what is extracted from spatial mapping data. Defaults to dispersion. Valid entries are:
            dispersion
            MDC
            EDC
        Selecting MDC/EDC will rapidly increase calculation time.

    E : float (optional)
        Energy of MDCs to extract. Defaults to 0.

    dE : float (optional)
        MDC Integration range (represents the total range, i.e. integrates over +/- dE/2). Defaults to 0.

    k : float (optional)
        k or theta_par value of EDCs to extract. Defaults to 0.

    dk : float (optional)
        EDC integration range (represents the total range, i.e. integrates over +/- dk/2). Defaults to 0.

    scale : Boolean (optional)
        Whether to apply standard scaling to data (i.e. center all values in each dimension around zero with unit
        standard deviation). Defaults to False.

    norm : Boolean (optional)
        Whether to normalise the data at each spatial position. Defaults to False.

    Returns
    ------------
    df : pd.DataFrame
        The spatial mapping data represented as a tabular pandas dataframe.

    Examples
    ------------
    from peaks import *

    SM = load(&#39;SM.ibw&#39;)

    SM_df1 = SM.xarray_to_ML_format()  # Get spatial mapping data in a tabular pandas dataframe format

    SM_df2 = SM.xarray_to_ML_format(extract=&#39;MDC&#39;, E=73.42, dE=0.02)  # Get spatial mapping MDCs in a tabular pandas
                                                                        dataframe format

    &#34;&#34;&#34;

    # List to store spatial mapping data
    sample_data = []

    # Loop through spatial positions
    x1_vals = data.x1.data
    x2_vals = data.x2.data
    for i in range(len(x1_vals)):
        for j in range(len(x2_vals)):
            current_disp = data.isel(x1=i).isel(x2=j)

            # Apply normalisation at each spatial position if requested
            if norm:
                current_disp = current_disp.norm()

            # If we want to represent each feature to be a full dispersion
            if extract == &#39;dispersion&#39;:
                # Get total number of dimensions (pixels) of dispersion
                dimensions = 1
                for coord_dim in current_disp.shape:
                    dimensions = dimensions * coord_dim

                # Reshape current dispersion into tabular form and append row to data
                reshaped_disp = current_disp.data.reshape(dimensions)
                sample_data.append(reshaped_disp)

            # If we want to represent each feature to be an MDC
            elif extract == &#39;MDC&#39;:
                # Extract MDC from dispersion
                current_MDC = current_disp.MDC(E=E, dE=dE)
                sample_data.append(current_MDC.data)

            # If we want to represent each feature to be an EDC
            elif extract == &#39;EDC&#39;:
                # Extract EDC from dispersion
                current_EDC = current_disp.EDC(k=k, dk=dk)
                sample_data.append(current_EDC.data)

            # Else user has entered an invalid method argument
            else:
                raise Exception(&#39;Method must be dispersion, MDC or EDC&#39;)

    # Create tabular pandas dataframe
    df = pd.DataFrame(data=np.array(sample_data))

    if scale:
        # Apply optional standard scaling (center all values in each dimension around zero with unit standard deviation)
        std_scaler = StandardScaler()
        df = pd.DataFrame(data=std_scaler.fit_transform(df))

    return df


def perform_k_means(data, k=3, n_init=&#34;auto&#34;):
    &#34;&#34;&#34;Perform k-means clustering using scikit-learn.

    Parameters
    ------------
    data : pd.DataFrame
        The data represented as a tabular pandas dataframe to perform clustering analysis on.

    k : int (optional)
        Number of clusters. Defaults to 3.

    n_init : int, string (optional)
        Number of times the k-means algorithm will be run with different centroid seeds. The final results will be the
        best output of n_init consecutive runs in terms of inertia. Required since the kmeans algorithm can fall into
        local minima, so repeats are required to check for this. Defaults to &#39;auto&#39; (note: if an outdated scikit-learn
        package is installed where &#39;auto&#39; is not yet implemented, an error will arise. In this case set n_init=10, or
        similar).

    Returns
    ------------
    model : sklearn.cluster._kmeans.KMeans
        K-means clustering analysis model information.

    labels : np.ndarray
        Array of spatially-dependent cluster assignments.

    Examples
    ------------
    from peaks import *

    SM = load(&#39;SM.ibw&#39;)

    SM_df = SM.xarray_to_ML_format()  # Get spatial mapping data in a tabular pandas dataframe format

    model1, labels1 = perform_k_means(data=SM_df)  # Perform k-means clustering analysis for 3 clusters

    model2, labels2 = perform_k_means(data=SM_df, k=4)  # Perform k-means clustering analysis for 4 clusters

    &#34;&#34;&#34;

    model = KMeans(n_clusters=k, n_init=n_init)  # Create a k-means clustering model
    model.fit(data)  # Fit the model to the sampling data
    labels = model.predict(data)  # Predict the labels of the samples, i.e. which cluster they belong to

    return model, labels


@add_methods(xr.DataArray)
def clusters_explore(data, cluster_range=range(1, 7), n_init=&#34;auto&#34;, use_PCA=True, PCs=3, extract=&#39;dispersion&#39;, E=0,
                     dE=0, k=0, dk=0, scale=False, norm=False):
    &#34;&#34;&#34;Perform an exploratory k-means clustering analysis on a spatial map for a range of number of clusters.

    Parameters
    ------------
    data : xr.DataArray
        The spatial mapping data to perform an exploratory k-means clustering analysis on.

    cluster_range : range (optional)
        Range of number of clusters to perform k-means clustering analysis for. Defaults to range(1,7).

    n_init : int, string (optional)
        Number of times the k-means algorithm will be run with different centroid seeds. The final results will be the
        best output of n_init consecutive runs in terms of inertia. Required since the kmeans algorithm can fall into
        local minima, so repeats are required to check for this. Defaults to &#39;auto&#39; (note: if an outdated scikit-learn
        package is installed where &#39;auto&#39; is not yet implemented, an error will arise. In this case set n_init=10, or
        similar).

    use_PCA : Boolean (optional)
        Whether to apply a principal component analysis to the data. Defaults to True.

    PCs: int (optional)
        Number of principal components. Defaults to 3.

    extract : str (optional)
        Determines what is extracted from spatial mapping data. Defaults to dispersion. Valid entries are:
            dispersion
            MDC
            EDC
        Selecting MDC/EDC will rapidly increase calculation time.

    E : float (optional)
        Energy of MDCs to extract. Defaults to 0.

    dE : float (optional)
        MDC Integration range (represents the total range, i.e. integrates over +/- dE/2). Defaults to 0.

    k : float (optional)
        k or theta_par value of EDCs to extract. Defaults to 0.

    dk : float (optional)
        EDC integration range (represents the total range, i.e. integrates over +/- dk/2). Defaults to 0.

    scale : Boolean (optional)
        Whether to apply standard scaling to data (i.e. center all values in each dimension around zero with unit
        standard deviation). Defaults to False.

    norm : Boolean (optional)
        Whether to normalise the data at each spatial position. Defaults to False.

    Examples
    ------------
    from peaks import *

    SM = load(&#39;SM.ibw&#39;)

    SM.clusters_explore()  # Perform an exploratory k-means clustering analysis for numbers of clusters ranging from
                            1 to 6, applying a principal component analysis to the spatial mapping data using 3
                            principal components

    SM.clusters_explore(cluster_range=range(1,11), use_PCA=False, extract=&#39;MDC&#39;, E=73.42, dE=0.02)  # Perform an
                            exploratory k-means clustering analysis on spatial mapping MDCs for numbers of clusters
                            ranging from 1 to 10

    &#34;&#34;&#34;

    # Prevent unwanted overwriting of original data
    data = data.copy(deep=True)

    # Represent spatial mapping data as a tabular pandas dataframe
    df = data.xarray_to_ML_format(extract=extract, E=E, dE=dE, k=k, dk=dk, scale=scale, norm=norm)

    # Perform principal component analysis if PCA is True
    if use_PCA:
        pca = PCA(n_components=PCs)  # Define PCA model
        principal_components = pca.fit_transform(df)  # Fit PCA model to data and get principal components
        df = pd.DataFrame(data=principal_components)  # Get principal components in tabular pandas dataframe format

    # Perform k-means clustering analysis for the range of number of clusters (k) requested
    k_titles = [&#39;k=&#39; + str(k) for k in cluster_range]  # Titles for plots
    inertias = []  # Empty list to store model inertias (a metric that defines spread of a cluster)
    classification_maps = []  # Empty list to store classification maps (spatial maps of cluster labels)
    for num_clusters in tqdm(cluster_range, desc=&#39;Calculating&#39;, colour=&#39;CYAN&#39;):
        model, labels = perform_k_means(data=df, k=num_clusters, n_init=n_init)  # Perform k-means clustering
        inertias.append(model.inertia_)
        classification_map_data = labels.reshape(len(data.x1), len(data.x2))  # Reshape 1D labels to 2D
        classification_map = xr.DataArray(classification_map_data, dims=(&#34;x1&#34;, &#34;x2&#34;),
                                          coords={&#34;x1&#34;: data.x1, &#34;x2&#34;: data.x2})
        classification_maps.append(classification_map)

    inertias_xarray = xr.DataArray(inertias, dims=&#34;num_clusters&#34;,
                                   coords={&#34;num_clusters&#34;: cluster_range})  # Create xarray of inertias

    # Find optimal number of clusters
    for item in abs(inertias_xarray.differentiate(&#39;num_clusters&#39;)).norm():
        # If rate of decrease in inertia is below 20% of the initial decrease, found optimal number of clusters
        if float(item) &lt; 0.2:
            recommended_num_clusters = int(item.num_clusters)
            break
        # Else continue through loop
        else:
            recommended_num_clusters = int(item.num_clusters)

    # Remove progress bar
    clear_output()

    # Plot model inertia against number of clusters, and indicate recommended number of clusters
    inertias_xarray.plot(marker=&#39;o&#39;, figsize=(15, 4))
    plt.xlabel(&#39;Number of clusters (k)&#39;)
    plt.ylabel(&#39;Inertia&#39;)
    plt.axvline(recommended_num_clusters, c=&#39;black&#39;, linestyle=&#39;--&#39;)
    plt.title(r&#39;Optimal k$\approx$&#39; + str(recommended_num_clusters))
    plt.show()

    # Plot classification map dependence on number of clusters
    plot_grid(classification_maps, ncols=3, titles=k_titles, cmap=&#39;cividis&#39;, y=&#39;x2&#39;)
    plt.show()


@add_methods(xr.DataArray)
def clusters(data, num_clusters=3, n_init=&#34;auto&#34;, use_PCA=True, PCs=3, extract=&#39;dispersion&#39;, E=0, dE=0, k=0, dk=0,
             scale=False, norm=False, robust=False, vmin=None, vmax=None):
    &#34;&#34;&#34;Perform a k-means clustering analysis on a spatial map.

    Parameters
    ------------
    data : xr.DataArray
        The spatial mapping data to perform an exploratory k-means clustering analysis on.

    num_clusters : int (optional)
        Number of clusters to perform k-means clustering analysis for. Defaults to 3.

    n_init : int, string (optional)
        Number of times the k-means algorithm will be run with different centroid seeds. The final results will be the
        best output of n_init consecutive runs in terms of inertia. Required since the kmeans algorithm can fall into
        local minima, so repeats are required to check for this. Defaults to &#39;auto&#39; (note: if an outdated scikit-learn
        package is installed where &#39;auto&#39; is not yet implemented, an error will arise. In this case set n_init=10, or
        similar).

    use_PCA : Boolean (optional)
        Whether to apply a principal component analysis to the data. Defaults to True.

    PCs: int (optional)
        Number of principal components. Defaults to 3.

    extract : str (optional)
        Determines what is extracted from spatial mapping data. Defaults to dispersion. Valid entries are:
            dispersion
            MDC
            EDC
        Selecting MDC/EDC will rapidly increase calculation time.

    E : float (optional)
        Energy of MDCs to extract. Defaults to 0.

    dE : float (optional)
        MDC Integration range (represents the total range, i.e. integrates over +/- dE/2). Defaults to 0.

    k : float (optional)
        k or theta_par value of EDCs to extract. Defaults to 0.

    dk : float (optional)
        EDC integration range (represents the total range, i.e. integrates over +/- dk/2). Defaults to 0.

    scale : Boolean (optional)
        Whether to apply standard scaling to data (i.e. center all values in each dimension around zero with unit
        standard deviation). Defaults to False.

    norm : Boolean (optional)
        Whether to normalise the data at each spatial position. Defaults to False.

    robust : Boolean (optional)
        Whether the argument robust=True is passed to the plots. Defaults to False.

    vmin : float (optional)
        Matplotlib vmin value used in plots of dispersions. Defaults to None.

    vmax : float (optional)
        Matplotlib vmax value used in plots of dispersions. Defaults to None.

    Returns
    ------------
    classification_map : xr.DataArray
        Spatial map of cluster labels.

    cluster_center_disps : list
        List of dispersions for each cluster center.

    Examples
    ------------
    from peaks import *

    SM = load(&#39;SM.ibw&#39;)

    classification_map1, cluster_center_disps1 = SM.clusters(num_clusters=3, PCs=4)  # Perform a k-means clustering
                        analysis using 3 clusters, applying a principal component analysis to the spatial mapping data
                        using 4 principal components

    classification_map2, cluster_center_disps2 = SM.clusters(num_clusters=3, use_PCA=False, extract=&#39;MDC&#39;, E=73.42,
                        dE=0.02)  # Perform a k-means clustering analysis using 3 clusters on spatial mapping MDCs

    &#34;&#34;&#34;

    # Prevent unwanted overwriting of original data
    data = data.copy(deep=True)

    # Represent spatial mapping data as a tabular pandas dataframe
    df = data.xarray_to_ML_format(extract=extract, E=E, dE=dE, k=k, dk=dk, scale=scale, norm=norm)

    # Perform principal component analysis if PCA is True
    if use_PCA:
        pca = PCA(n_components=PCs)  # Define PCA model
        principal_components = pca.fit_transform(df)  # Fit PCA model to data and get principal components
        df = pd.DataFrame(data=principal_components)  # Get principal components in tabular pandas dataframe format

    # Perform k-means clustering analysis for the number of clusters (k) requested
    model, labels = perform_k_means(data=df, k=num_clusters, n_init=n_init)
    classification_map_data = labels.reshape(len(data.x1), len(data.x2))  # Reshape 1D labels to 2D
    classification_map = xr.DataArray(classification_map_data, dims=(&#34;x1&#34;, &#34;x2&#34;), coords={&#34;x1&#34;: data.x1, &#34;x2&#34;: data.x2})

    # Extract cluster center of each cluster (not done using cluster centers so that we can get dispersions if MDC/EDC
    # extraction is used)
    cluster_average_disps = []  # Empty list to store average dispersions
    # Loop through cluster labels
    for cluster in range(num_clusters):
        current_disps = []
        # Loop through spatial positions
        for i in range(len(data.x1)):
            for j in range(len(data.x2)):
                # If current spatial position is assigned to current cluster
                if float(classification_map.isel(x1=i).isel(x2=j)) == cluster:
                    current_disps.append(data.isel(x1=i).isel(x2=j).copy(deep=True))
        # Sum dispersions with the same cluster label
        total_disp = current_disps[0]
        for i in range(1, len(current_disps)):
            total_disp += current_disps[i]
        cluster_average_disps.append(total_disp / len(current_disps))

    # Find maximum vmax of the averaged cluster dispersions so intensity variations between terminations can be observed
    if robust == False and vmax == None:
        max_cluster_disps_vmax = 0
        for disp in cluster_average_disps:
            if float(disp.max()) &gt; max_cluster_disps_vmax:
                max_cluster_disps_vmax = float(disp.max())

    # Plot integrated spectral weight and k-means clustering labels spatial maps
    fig, axes = plt.subplots(figsize=(12, 5), ncols=2)
    data.tot().plot(ax=axes[0], cmap=&#39;cividis&#39;, y=&#39;x2&#39;, cbar_kwargs={&#39;label&#39;: &#39;Intensity&#39;})
    classification_map.plot(ax=axes[1], cmap=&#39;cividis&#39;, y=&#39;x2&#39;,
                            cbar_kwargs={&#39;ticks&#39;: range(num_clusters), &#39;label&#39;: &#39;Cluster&#39;})
    axes[0].set_title(&#39;Integrated spectral weight&#39;)
    axes[1].set_title(&#39;k-means clustering (k=&#39; + str(num_clusters) + &#39;)&#39;)
    plt.tight_layout()
    plt.show()

    # Plot average dispersion of each cluster
    cluster_average_titles = [&#39;Cluster &#39; + str(i) for i in range(num_clusters)]  # Titles for plots
    if vmax != None:
        plot_grid(cluster_average_disps, titles=cluster_average_titles, cmap=&#39;binary&#39;, y=&#39;eV&#39;, ncols=num_clusters,
                  vmin=vmin, vmax=vmax, cbar_kwargs={&#39;label&#39;: None})
    else:
        if robust == False:
            plot_grid(cluster_average_disps, titles=cluster_average_titles, cmap=&#39;binary&#39;, y=&#39;eV&#39;, ncols=num_clusters,
                      vmin=vmin, vmax=max_cluster_disps_vmax, cbar_kwargs={&#39;label&#39;: None})
        else:
            plot_grid(cluster_average_disps, titles=cluster_average_titles, cmap=&#39;binary&#39;, y=&#39;eV&#39;, ncols=num_clusters,
                      robust=True, cbar_kwargs={&#39;label&#39;: None})
    plt.tight_layout()
    plt.show()

    # If a PCA was used on full dispersions, plot also the reconstructed cluster centers
    if use_PCA == True and extract == &#39;dispersion&#39;:
        # Get non-spatial coordinates
        coords = list(data.dims)
        coords.remove(&#39;x1&#39;)
        coords.remove(&#39;x2&#39;)

        # Get reconstructed cluster center dispersions from reduced dimensionality dataset
        reconstructed_cluster_centers = pca.inverse_transform(model.cluster_centers_).reshape(num_clusters, len(
            data.coords[coords[0]]), len(data.coords[coords[1]]))
        reconstructed_cluster_centers_disps = []
        for center in reconstructed_cluster_centers:
            disp_xarray = xr.DataArray(center, dims=(coords[0], coords[1]),
                                       coords={coords[0]: data.coords[coords[0]], coords[1]: data.coords[coords[1]]})
            reconstructed_cluster_centers_disps.append(disp_xarray)

        # Find maximum vmax of the reconstructed cluster center dispersions so intensity variations between
        # terminations can be observed
        if robust == False and vmax == None:
            max_reconstructed_cluster_disps_vmax = 0
            for disp in reconstructed_cluster_centers_disps:
                if float(disp.max()) &gt; max_reconstructed_cluster_disps_vmax:
                    max_reconstructed_cluster_disps_vmax = float(disp.max())

        # reconstructed cluster center dispersions
        reconstructed_cluster_centers_titles = [&#39;Cluster &#39; + str(i) + &#39; (reconstructed)&#39; for i in
                                                range(num_clusters)]  # Titles for plots
        if vmax != None:
            plot_grid(reconstructed_cluster_centers_disps, titles=reconstructed_cluster_centers_titles, cmap=&#39;binary&#39;,
                      y=&#39;eV&#39;, ncols=num_clusters, vmin=vmin, vmax=vmax, cbar_kwargs={&#39;label&#39;: None})
        else:
            if robust == False:
                plot_grid(reconstructed_cluster_centers_disps, titles=reconstructed_cluster_centers_titles,
                          cmap=&#39;binary&#39;, y=&#39;eV&#39;, ncols=num_clusters, vmin=vmin,
                          vmax=max_reconstructed_cluster_disps_vmax, cbar_kwargs={&#39;label&#39;: None})
            else:
                plot_grid(reconstructed_cluster_centers_disps, titles=reconstructed_cluster_centers_titles,
                          cmap=&#39;binary&#39;, y=&#39;eV&#39;, ncols=num_clusters, robust=True, cbar_kwargs={&#39;label&#39;: None})
        plt.tight_layout()
        plt.show()

    return classification_map, cluster_average_disps


@add_methods(xr.DataArray)
def PCA_explore(data, PCs_range=range(1, 6), threshold=0.95, extract=&#39;dispersion&#39;, E=0, dE=0, k=0, dk=0, scale=False,
                norm=False):
    &#34;&#34;&#34;Perform an exploratory principal component analysis on a spatial map for a range of principal components.

    Parameters
    ------------
    data : xr.DataArray
        The spatial mapping data to perform an exploratory principal component analysis on.

    PCs_range : range (optional)
        Range of number of principal components to perform PCA for. Defaults to range(1,6).

    threshold : float (optional)
        Required threshold for the explained variance fraction of the dimensionally reduced dataset. Defaults to 0.95.

    extract : str (optional)
        Determines what is extracted from spatial mapping data. Defaults to dispersion. Valid entries are:
            dispersion
            MDC
            EDC
        Selecting MDC/EDC will rapidly increase calculation time.

    E : float (optional)
        Energy of MDCs to extract. Defaults to 0.

    dE : float (optional)
        MDC Integration range (represents the total range, i.e. integrates over +/- dE/2). Defaults to 0.

    k : float (optional)
        k or theta_par value of EDCs to extract. Defaults to 0.

    dk : float (optional)
        EDC integration range (represents the total range, i.e. integrates over +/- dk/2). Defaults to 0.

    scale : Boolean (optional)
        Whether to apply standard scaling to data (i.e. center all values in each dimension around zero with unit
        standard deviation). Defaults to False.

    norm : Boolean (optional)
        Whether to normalise the data at each spatial position. Defaults to False.

    Examples
    ------------
    from peaks import *

    SM = load(&#39;SM.ibw&#39;)

    SM.PCA_explore()  # Perform an exploratory PCA on spatial mapping data for numbers of principal components
                        ranging from 1 to 5

    SM.PCA_explore(PCs_range=range(1,11), extract=&#39;MDC&#39;, E=73.42, dE=0.02)  # Perform an exploratory PCA on spatial
                        mapping MDCs for numbers of principal components ranging from 1 to 10

    &#34;&#34;&#34;

    # Prevent unwanted overwriting of original data
    data = data.copy(deep=True)

    # Represent spatial mapping data as a tabular pandas dataframes
    df = data.xarray_to_ML_format(extract=extract, E=E, dE=dE, k=k, dk=dk, scale=scale, norm=norm)

    summed_var_ratio = []  # Empty list to store explained variance fractions

    # Loop through the range of principal components to test, and perform PCA at each
    for num_principal_components in tqdm(PCs_range, desc=&#39;Calculating&#39;, colour=&#39;CYAN&#39;):
        model = PCA(n_components=num_principal_components)  # Define PCA model
        model.fit(df)  # Fit PCA model to dataset
        summed_var_ratio.append(
            np.sum(model.explained_variance_ratio_))  # Append the total explained variance fractions of each model
    clear_output()  # Remove progress bar

    # Check minimum number of principal axes required to exceed explained variance fraction threshold
    num_principal_axes = &#39;Unknown&#39;
    for i, item in enumerate(summed_var_ratio):
        if item &gt;= threshold:
            num_principal_axes = PCs_range[i]
            break

    # If explained variance could not exceed threshold, display a warning
    if num_principal_axes == &#39;Unknown&#39;:
        analysis_warning(&#39;Explained variance could not exceed threshold. Either reduce threshold or increase PCA_range&#39;,
                         title=&#39;Analysis info&#39;, warn_type=&#39;danger&#39;)

    # Plot results of exploratory PCA, and suggest minimum required number of principal axes (if the explained
    # variance threshold has been exceeded)
    plt.figure(figsize=(8, 5))
    plt.plot(PCs_range, summed_var_ratio, &#39;o-&#39;)
    plt.axhline(threshold, c=&#39;black&#39;, linestyle=&#39;--&#39;)
    if num_principal_axes != &#39;Unknown&#39;:
        plt.axvline(num_principal_axes, c=&#39;black&#39;, linestyle=&#39;--&#39;)
    plt.xlabel(&#39;Principal axes&#39;)
    plt.ylabel(&#39;Explained variance fraction&#39;)
    plt.title(r&#39;Minimum number of required principal axes$=$&#39; + str(num_principal_axes))
    plt.show()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="peaks.ML.clustering.PCA_explore"><code class="name flex">
<span>def <span class="ident">PCA_explore</span></span>(<span>data, PCs_range=range(1, 6), threshold=0.95, extract='dispersion', E=0, dE=0, k=0, dk=0, scale=False, norm=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Perform an exploratory principal component analysis on a spatial map for a range of principal components.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>xr.DataArray</code></dt>
<dd>The spatial mapping data to perform an exploratory principal component analysis on.</dd>
<dt><strong><code>PCs_range</code></strong> :&ensp;<code>range (optional)</code></dt>
<dd>Range of number of principal components to perform PCA for. Defaults to range(1,6).</dd>
<dt><strong><code>threshold</code></strong> :&ensp;<code>float (optional)</code></dt>
<dd>Required threshold for the explained variance fraction of the dimensionally reduced dataset. Defaults to 0.95.</dd>
<dt><strong><code>extract</code></strong> :&ensp;<code>str (optional)</code></dt>
<dd>Determines what is extracted from spatial mapping data. Defaults to dispersion. Valid entries are:
dispersion
MDC
EDC
Selecting MDC/EDC will rapidly increase calculation time.</dd>
<dt><strong><code>E</code></strong> :&ensp;<code>float (optional)</code></dt>
<dd>Energy of MDCs to extract. Defaults to 0.</dd>
<dt><strong><code>dE</code></strong> :&ensp;<code>float (optional)</code></dt>
<dd>MDC Integration range (represents the total range, i.e. integrates over +/- dE/2). Defaults to 0.</dd>
<dt><strong><code>k</code></strong> :&ensp;<code>float (optional)</code></dt>
<dd>k or theta_par value of EDCs to extract. Defaults to 0.</dd>
<dt><strong><code>dk</code></strong> :&ensp;<code>float (optional)</code></dt>
<dd>EDC integration range (represents the total range, i.e. integrates over +/- dk/2). Defaults to 0.</dd>
<dt><strong><code>scale</code></strong> :&ensp;<code>Boolean (optional)</code></dt>
<dd>Whether to apply standard scaling to data (i.e. center all values in each dimension around zero with unit
standard deviation). Defaults to False.</dd>
<dt><strong><code>norm</code></strong> :&ensp;<code>Boolean (optional)</code></dt>
<dd>Whether to normalise the data at each spatial position. Defaults to False.</dd>
</dl>
<h2 id="examples">Examples</h2>
<p>from peaks import *</p>
<p>SM = load('SM.ibw')</p>
<p>SM.PCA_explore()
# Perform an exploratory PCA on spatial mapping data for numbers of principal components
ranging from 1 to 5</p>
<p>SM.PCA_explore(PCs_range=range(1,11), extract='MDC', E=73.42, dE=0.02)
# Perform an exploratory PCA on spatial
mapping MDCs for numbers of principal components ranging from 1 to 10</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@add_methods(xr.DataArray)
def PCA_explore(data, PCs_range=range(1, 6), threshold=0.95, extract=&#39;dispersion&#39;, E=0, dE=0, k=0, dk=0, scale=False,
                norm=False):
    &#34;&#34;&#34;Perform an exploratory principal component analysis on a spatial map for a range of principal components.

    Parameters
    ------------
    data : xr.DataArray
        The spatial mapping data to perform an exploratory principal component analysis on.

    PCs_range : range (optional)
        Range of number of principal components to perform PCA for. Defaults to range(1,6).

    threshold : float (optional)
        Required threshold for the explained variance fraction of the dimensionally reduced dataset. Defaults to 0.95.

    extract : str (optional)
        Determines what is extracted from spatial mapping data. Defaults to dispersion. Valid entries are:
            dispersion
            MDC
            EDC
        Selecting MDC/EDC will rapidly increase calculation time.

    E : float (optional)
        Energy of MDCs to extract. Defaults to 0.

    dE : float (optional)
        MDC Integration range (represents the total range, i.e. integrates over +/- dE/2). Defaults to 0.

    k : float (optional)
        k or theta_par value of EDCs to extract. Defaults to 0.

    dk : float (optional)
        EDC integration range (represents the total range, i.e. integrates over +/- dk/2). Defaults to 0.

    scale : Boolean (optional)
        Whether to apply standard scaling to data (i.e. center all values in each dimension around zero with unit
        standard deviation). Defaults to False.

    norm : Boolean (optional)
        Whether to normalise the data at each spatial position. Defaults to False.

    Examples
    ------------
    from peaks import *

    SM = load(&#39;SM.ibw&#39;)

    SM.PCA_explore()  # Perform an exploratory PCA on spatial mapping data for numbers of principal components
                        ranging from 1 to 5

    SM.PCA_explore(PCs_range=range(1,11), extract=&#39;MDC&#39;, E=73.42, dE=0.02)  # Perform an exploratory PCA on spatial
                        mapping MDCs for numbers of principal components ranging from 1 to 10

    &#34;&#34;&#34;

    # Prevent unwanted overwriting of original data
    data = data.copy(deep=True)

    # Represent spatial mapping data as a tabular pandas dataframes
    df = data.xarray_to_ML_format(extract=extract, E=E, dE=dE, k=k, dk=dk, scale=scale, norm=norm)

    summed_var_ratio = []  # Empty list to store explained variance fractions

    # Loop through the range of principal components to test, and perform PCA at each
    for num_principal_components in tqdm(PCs_range, desc=&#39;Calculating&#39;, colour=&#39;CYAN&#39;):
        model = PCA(n_components=num_principal_components)  # Define PCA model
        model.fit(df)  # Fit PCA model to dataset
        summed_var_ratio.append(
            np.sum(model.explained_variance_ratio_))  # Append the total explained variance fractions of each model
    clear_output()  # Remove progress bar

    # Check minimum number of principal axes required to exceed explained variance fraction threshold
    num_principal_axes = &#39;Unknown&#39;
    for i, item in enumerate(summed_var_ratio):
        if item &gt;= threshold:
            num_principal_axes = PCs_range[i]
            break

    # If explained variance could not exceed threshold, display a warning
    if num_principal_axes == &#39;Unknown&#39;:
        analysis_warning(&#39;Explained variance could not exceed threshold. Either reduce threshold or increase PCA_range&#39;,
                         title=&#39;Analysis info&#39;, warn_type=&#39;danger&#39;)

    # Plot results of exploratory PCA, and suggest minimum required number of principal axes (if the explained
    # variance threshold has been exceeded)
    plt.figure(figsize=(8, 5))
    plt.plot(PCs_range, summed_var_ratio, &#39;o-&#39;)
    plt.axhline(threshold, c=&#39;black&#39;, linestyle=&#39;--&#39;)
    if num_principal_axes != &#39;Unknown&#39;:
        plt.axvline(num_principal_axes, c=&#39;black&#39;, linestyle=&#39;--&#39;)
    plt.xlabel(&#39;Principal axes&#39;)
    plt.ylabel(&#39;Explained variance fraction&#39;)
    plt.title(r&#39;Minimum number of required principal axes$=$&#39; + str(num_principal_axes))
    plt.show()</code></pre>
</details>
</dd>
<dt id="peaks.ML.clustering.clusters"><code class="name flex">
<span>def <span class="ident">clusters</span></span>(<span>data, num_clusters=3, n_init='auto', use_PCA=True, PCs=3, extract='dispersion', E=0, dE=0, k=0, dk=0, scale=False, norm=False, robust=False, vmin=None, vmax=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Perform a k-means clustering analysis on a spatial map.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>xr.DataArray</code></dt>
<dd>The spatial mapping data to perform an exploratory k-means clustering analysis on.</dd>
<dt><strong><code>num_clusters</code></strong> :&ensp;<code>int (optional)</code></dt>
<dd>Number of clusters to perform k-means clustering analysis for. Defaults to 3.</dd>
<dt><strong><code>n_init</code></strong> :&ensp;<code>int, string (optional)</code></dt>
<dd>Number of times the k-means algorithm will be run with different centroid seeds. The final results will be the
best output of n_init consecutive runs in terms of inertia. Required since the kmeans algorithm can fall into
local minima, so repeats are required to check for this. Defaults to 'auto' (note: if an outdated scikit-learn
package is installed where 'auto' is not yet implemented, an error will arise. In this case set n_init=10, or
similar).</dd>
<dt><strong><code>use_PCA</code></strong> :&ensp;<code>Boolean (optional)</code></dt>
<dd>Whether to apply a principal component analysis to the data. Defaults to True.</dd>
<dt><strong><code>PCs</code></strong> :&ensp;<code>int (optional)</code></dt>
<dd>Number of principal components. Defaults to 3.</dd>
<dt><strong><code>extract</code></strong> :&ensp;<code>str (optional)</code></dt>
<dd>Determines what is extracted from spatial mapping data. Defaults to dispersion. Valid entries are:
dispersion
MDC
EDC
Selecting MDC/EDC will rapidly increase calculation time.</dd>
<dt><strong><code>E</code></strong> :&ensp;<code>float (optional)</code></dt>
<dd>Energy of MDCs to extract. Defaults to 0.</dd>
<dt><strong><code>dE</code></strong> :&ensp;<code>float (optional)</code></dt>
<dd>MDC Integration range (represents the total range, i.e. integrates over +/- dE/2). Defaults to 0.</dd>
<dt><strong><code>k</code></strong> :&ensp;<code>float (optional)</code></dt>
<dd>k or theta_par value of EDCs to extract. Defaults to 0.</dd>
<dt><strong><code>dk</code></strong> :&ensp;<code>float (optional)</code></dt>
<dd>EDC integration range (represents the total range, i.e. integrates over +/- dk/2). Defaults to 0.</dd>
<dt><strong><code>scale</code></strong> :&ensp;<code>Boolean (optional)</code></dt>
<dd>Whether to apply standard scaling to data (i.e. center all values in each dimension around zero with unit
standard deviation). Defaults to False.</dd>
<dt><strong><code>norm</code></strong> :&ensp;<code>Boolean (optional)</code></dt>
<dd>Whether to normalise the data at each spatial position. Defaults to False.</dd>
<dt><strong><code>robust</code></strong> :&ensp;<code>Boolean (optional)</code></dt>
<dd>Whether the argument robust=True is passed to the plots. Defaults to False.</dd>
<dt><strong><code>vmin</code></strong> :&ensp;<code>float (optional)</code></dt>
<dd>Matplotlib vmin value used in plots of dispersions. Defaults to None.</dd>
<dt><strong><code>vmax</code></strong> :&ensp;<code>float (optional)</code></dt>
<dd>Matplotlib vmax value used in plots of dispersions. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>classification_map</code></strong> :&ensp;<code>xr.DataArray</code></dt>
<dd>Spatial map of cluster labels.</dd>
<dt><strong><code>cluster_center_disps</code></strong> :&ensp;<code>list</code></dt>
<dd>List of dispersions for each cluster center.</dd>
</dl>
<h2 id="examples">Examples</h2>
<p>from peaks import *</p>
<p>SM = load('SM.ibw')</p>
<p>classification_map1, cluster_center_disps1 = SM.clusters(num_clusters=3, PCs=4)
# Perform a k-means clustering
analysis using 3 clusters, applying a principal component analysis to the spatial mapping data
using 4 principal components</p>
<p>classification_map2, cluster_center_disps2 = SM.clusters(num_clusters=3, use_PCA=False, extract='MDC', E=73.42,
dE=0.02)
# Perform a k-means clustering analysis using 3 clusters on spatial mapping MDCs</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@add_methods(xr.DataArray)
def clusters(data, num_clusters=3, n_init=&#34;auto&#34;, use_PCA=True, PCs=3, extract=&#39;dispersion&#39;, E=0, dE=0, k=0, dk=0,
             scale=False, norm=False, robust=False, vmin=None, vmax=None):
    &#34;&#34;&#34;Perform a k-means clustering analysis on a spatial map.

    Parameters
    ------------
    data : xr.DataArray
        The spatial mapping data to perform an exploratory k-means clustering analysis on.

    num_clusters : int (optional)
        Number of clusters to perform k-means clustering analysis for. Defaults to 3.

    n_init : int, string (optional)
        Number of times the k-means algorithm will be run with different centroid seeds. The final results will be the
        best output of n_init consecutive runs in terms of inertia. Required since the kmeans algorithm can fall into
        local minima, so repeats are required to check for this. Defaults to &#39;auto&#39; (note: if an outdated scikit-learn
        package is installed where &#39;auto&#39; is not yet implemented, an error will arise. In this case set n_init=10, or
        similar).

    use_PCA : Boolean (optional)
        Whether to apply a principal component analysis to the data. Defaults to True.

    PCs: int (optional)
        Number of principal components. Defaults to 3.

    extract : str (optional)
        Determines what is extracted from spatial mapping data. Defaults to dispersion. Valid entries are:
            dispersion
            MDC
            EDC
        Selecting MDC/EDC will rapidly increase calculation time.

    E : float (optional)
        Energy of MDCs to extract. Defaults to 0.

    dE : float (optional)
        MDC Integration range (represents the total range, i.e. integrates over +/- dE/2). Defaults to 0.

    k : float (optional)
        k or theta_par value of EDCs to extract. Defaults to 0.

    dk : float (optional)
        EDC integration range (represents the total range, i.e. integrates over +/- dk/2). Defaults to 0.

    scale : Boolean (optional)
        Whether to apply standard scaling to data (i.e. center all values in each dimension around zero with unit
        standard deviation). Defaults to False.

    norm : Boolean (optional)
        Whether to normalise the data at each spatial position. Defaults to False.

    robust : Boolean (optional)
        Whether the argument robust=True is passed to the plots. Defaults to False.

    vmin : float (optional)
        Matplotlib vmin value used in plots of dispersions. Defaults to None.

    vmax : float (optional)
        Matplotlib vmax value used in plots of dispersions. Defaults to None.

    Returns
    ------------
    classification_map : xr.DataArray
        Spatial map of cluster labels.

    cluster_center_disps : list
        List of dispersions for each cluster center.

    Examples
    ------------
    from peaks import *

    SM = load(&#39;SM.ibw&#39;)

    classification_map1, cluster_center_disps1 = SM.clusters(num_clusters=3, PCs=4)  # Perform a k-means clustering
                        analysis using 3 clusters, applying a principal component analysis to the spatial mapping data
                        using 4 principal components

    classification_map2, cluster_center_disps2 = SM.clusters(num_clusters=3, use_PCA=False, extract=&#39;MDC&#39;, E=73.42,
                        dE=0.02)  # Perform a k-means clustering analysis using 3 clusters on spatial mapping MDCs

    &#34;&#34;&#34;

    # Prevent unwanted overwriting of original data
    data = data.copy(deep=True)

    # Represent spatial mapping data as a tabular pandas dataframe
    df = data.xarray_to_ML_format(extract=extract, E=E, dE=dE, k=k, dk=dk, scale=scale, norm=norm)

    # Perform principal component analysis if PCA is True
    if use_PCA:
        pca = PCA(n_components=PCs)  # Define PCA model
        principal_components = pca.fit_transform(df)  # Fit PCA model to data and get principal components
        df = pd.DataFrame(data=principal_components)  # Get principal components in tabular pandas dataframe format

    # Perform k-means clustering analysis for the number of clusters (k) requested
    model, labels = perform_k_means(data=df, k=num_clusters, n_init=n_init)
    classification_map_data = labels.reshape(len(data.x1), len(data.x2))  # Reshape 1D labels to 2D
    classification_map = xr.DataArray(classification_map_data, dims=(&#34;x1&#34;, &#34;x2&#34;), coords={&#34;x1&#34;: data.x1, &#34;x2&#34;: data.x2})

    # Extract cluster center of each cluster (not done using cluster centers so that we can get dispersions if MDC/EDC
    # extraction is used)
    cluster_average_disps = []  # Empty list to store average dispersions
    # Loop through cluster labels
    for cluster in range(num_clusters):
        current_disps = []
        # Loop through spatial positions
        for i in range(len(data.x1)):
            for j in range(len(data.x2)):
                # If current spatial position is assigned to current cluster
                if float(classification_map.isel(x1=i).isel(x2=j)) == cluster:
                    current_disps.append(data.isel(x1=i).isel(x2=j).copy(deep=True))
        # Sum dispersions with the same cluster label
        total_disp = current_disps[0]
        for i in range(1, len(current_disps)):
            total_disp += current_disps[i]
        cluster_average_disps.append(total_disp / len(current_disps))

    # Find maximum vmax of the averaged cluster dispersions so intensity variations between terminations can be observed
    if robust == False and vmax == None:
        max_cluster_disps_vmax = 0
        for disp in cluster_average_disps:
            if float(disp.max()) &gt; max_cluster_disps_vmax:
                max_cluster_disps_vmax = float(disp.max())

    # Plot integrated spectral weight and k-means clustering labels spatial maps
    fig, axes = plt.subplots(figsize=(12, 5), ncols=2)
    data.tot().plot(ax=axes[0], cmap=&#39;cividis&#39;, y=&#39;x2&#39;, cbar_kwargs={&#39;label&#39;: &#39;Intensity&#39;})
    classification_map.plot(ax=axes[1], cmap=&#39;cividis&#39;, y=&#39;x2&#39;,
                            cbar_kwargs={&#39;ticks&#39;: range(num_clusters), &#39;label&#39;: &#39;Cluster&#39;})
    axes[0].set_title(&#39;Integrated spectral weight&#39;)
    axes[1].set_title(&#39;k-means clustering (k=&#39; + str(num_clusters) + &#39;)&#39;)
    plt.tight_layout()
    plt.show()

    # Plot average dispersion of each cluster
    cluster_average_titles = [&#39;Cluster &#39; + str(i) for i in range(num_clusters)]  # Titles for plots
    if vmax != None:
        plot_grid(cluster_average_disps, titles=cluster_average_titles, cmap=&#39;binary&#39;, y=&#39;eV&#39;, ncols=num_clusters,
                  vmin=vmin, vmax=vmax, cbar_kwargs={&#39;label&#39;: None})
    else:
        if robust == False:
            plot_grid(cluster_average_disps, titles=cluster_average_titles, cmap=&#39;binary&#39;, y=&#39;eV&#39;, ncols=num_clusters,
                      vmin=vmin, vmax=max_cluster_disps_vmax, cbar_kwargs={&#39;label&#39;: None})
        else:
            plot_grid(cluster_average_disps, titles=cluster_average_titles, cmap=&#39;binary&#39;, y=&#39;eV&#39;, ncols=num_clusters,
                      robust=True, cbar_kwargs={&#39;label&#39;: None})
    plt.tight_layout()
    plt.show()

    # If a PCA was used on full dispersions, plot also the reconstructed cluster centers
    if use_PCA == True and extract == &#39;dispersion&#39;:
        # Get non-spatial coordinates
        coords = list(data.dims)
        coords.remove(&#39;x1&#39;)
        coords.remove(&#39;x2&#39;)

        # Get reconstructed cluster center dispersions from reduced dimensionality dataset
        reconstructed_cluster_centers = pca.inverse_transform(model.cluster_centers_).reshape(num_clusters, len(
            data.coords[coords[0]]), len(data.coords[coords[1]]))
        reconstructed_cluster_centers_disps = []
        for center in reconstructed_cluster_centers:
            disp_xarray = xr.DataArray(center, dims=(coords[0], coords[1]),
                                       coords={coords[0]: data.coords[coords[0]], coords[1]: data.coords[coords[1]]})
            reconstructed_cluster_centers_disps.append(disp_xarray)

        # Find maximum vmax of the reconstructed cluster center dispersions so intensity variations between
        # terminations can be observed
        if robust == False and vmax == None:
            max_reconstructed_cluster_disps_vmax = 0
            for disp in reconstructed_cluster_centers_disps:
                if float(disp.max()) &gt; max_reconstructed_cluster_disps_vmax:
                    max_reconstructed_cluster_disps_vmax = float(disp.max())

        # reconstructed cluster center dispersions
        reconstructed_cluster_centers_titles = [&#39;Cluster &#39; + str(i) + &#39; (reconstructed)&#39; for i in
                                                range(num_clusters)]  # Titles for plots
        if vmax != None:
            plot_grid(reconstructed_cluster_centers_disps, titles=reconstructed_cluster_centers_titles, cmap=&#39;binary&#39;,
                      y=&#39;eV&#39;, ncols=num_clusters, vmin=vmin, vmax=vmax, cbar_kwargs={&#39;label&#39;: None})
        else:
            if robust == False:
                plot_grid(reconstructed_cluster_centers_disps, titles=reconstructed_cluster_centers_titles,
                          cmap=&#39;binary&#39;, y=&#39;eV&#39;, ncols=num_clusters, vmin=vmin,
                          vmax=max_reconstructed_cluster_disps_vmax, cbar_kwargs={&#39;label&#39;: None})
            else:
                plot_grid(reconstructed_cluster_centers_disps, titles=reconstructed_cluster_centers_titles,
                          cmap=&#39;binary&#39;, y=&#39;eV&#39;, ncols=num_clusters, robust=True, cbar_kwargs={&#39;label&#39;: None})
        plt.tight_layout()
        plt.show()

    return classification_map, cluster_average_disps</code></pre>
</details>
</dd>
<dt id="peaks.ML.clustering.clusters_explore"><code class="name flex">
<span>def <span class="ident">clusters_explore</span></span>(<span>data, cluster_range=range(1, 7), n_init='auto', use_PCA=True, PCs=3, extract='dispersion', E=0, dE=0, k=0, dk=0, scale=False, norm=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Perform an exploratory k-means clustering analysis on a spatial map for a range of number of clusters.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>xr.DataArray</code></dt>
<dd>The spatial mapping data to perform an exploratory k-means clustering analysis on.</dd>
<dt><strong><code>cluster_range</code></strong> :&ensp;<code>range (optional)</code></dt>
<dd>Range of number of clusters to perform k-means clustering analysis for. Defaults to range(1,7).</dd>
<dt><strong><code>n_init</code></strong> :&ensp;<code>int, string (optional)</code></dt>
<dd>Number of times the k-means algorithm will be run with different centroid seeds. The final results will be the
best output of n_init consecutive runs in terms of inertia. Required since the kmeans algorithm can fall into
local minima, so repeats are required to check for this. Defaults to 'auto' (note: if an outdated scikit-learn
package is installed where 'auto' is not yet implemented, an error will arise. In this case set n_init=10, or
similar).</dd>
<dt><strong><code>use_PCA</code></strong> :&ensp;<code>Boolean (optional)</code></dt>
<dd>Whether to apply a principal component analysis to the data. Defaults to True.</dd>
<dt><strong><code>PCs</code></strong> :&ensp;<code>int (optional)</code></dt>
<dd>Number of principal components. Defaults to 3.</dd>
<dt><strong><code>extract</code></strong> :&ensp;<code>str (optional)</code></dt>
<dd>Determines what is extracted from spatial mapping data. Defaults to dispersion. Valid entries are:
dispersion
MDC
EDC
Selecting MDC/EDC will rapidly increase calculation time.</dd>
<dt><strong><code>E</code></strong> :&ensp;<code>float (optional)</code></dt>
<dd>Energy of MDCs to extract. Defaults to 0.</dd>
<dt><strong><code>dE</code></strong> :&ensp;<code>float (optional)</code></dt>
<dd>MDC Integration range (represents the total range, i.e. integrates over +/- dE/2). Defaults to 0.</dd>
<dt><strong><code>k</code></strong> :&ensp;<code>float (optional)</code></dt>
<dd>k or theta_par value of EDCs to extract. Defaults to 0.</dd>
<dt><strong><code>dk</code></strong> :&ensp;<code>float (optional)</code></dt>
<dd>EDC integration range (represents the total range, i.e. integrates over +/- dk/2). Defaults to 0.</dd>
<dt><strong><code>scale</code></strong> :&ensp;<code>Boolean (optional)</code></dt>
<dd>Whether to apply standard scaling to data (i.e. center all values in each dimension around zero with unit
standard deviation). Defaults to False.</dd>
<dt><strong><code>norm</code></strong> :&ensp;<code>Boolean (optional)</code></dt>
<dd>Whether to normalise the data at each spatial position. Defaults to False.</dd>
</dl>
<h2 id="examples">Examples</h2>
<p>from peaks import *</p>
<p>SM = load('SM.ibw')</p>
<p>SM.clusters_explore()
# Perform an exploratory k-means clustering analysis for numbers of clusters ranging from
1 to 6, applying a principal component analysis to the spatial mapping data using 3
principal components</p>
<p>SM.clusters_explore(cluster_range=range(1,11), use_PCA=False, extract='MDC', E=73.42, dE=0.02)
# Perform an
exploratory k-means clustering analysis on spatial mapping MDCs for numbers of clusters
ranging from 1 to 10</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@add_methods(xr.DataArray)
def clusters_explore(data, cluster_range=range(1, 7), n_init=&#34;auto&#34;, use_PCA=True, PCs=3, extract=&#39;dispersion&#39;, E=0,
                     dE=0, k=0, dk=0, scale=False, norm=False):
    &#34;&#34;&#34;Perform an exploratory k-means clustering analysis on a spatial map for a range of number of clusters.

    Parameters
    ------------
    data : xr.DataArray
        The spatial mapping data to perform an exploratory k-means clustering analysis on.

    cluster_range : range (optional)
        Range of number of clusters to perform k-means clustering analysis for. Defaults to range(1,7).

    n_init : int, string (optional)
        Number of times the k-means algorithm will be run with different centroid seeds. The final results will be the
        best output of n_init consecutive runs in terms of inertia. Required since the kmeans algorithm can fall into
        local minima, so repeats are required to check for this. Defaults to &#39;auto&#39; (note: if an outdated scikit-learn
        package is installed where &#39;auto&#39; is not yet implemented, an error will arise. In this case set n_init=10, or
        similar).

    use_PCA : Boolean (optional)
        Whether to apply a principal component analysis to the data. Defaults to True.

    PCs: int (optional)
        Number of principal components. Defaults to 3.

    extract : str (optional)
        Determines what is extracted from spatial mapping data. Defaults to dispersion. Valid entries are:
            dispersion
            MDC
            EDC
        Selecting MDC/EDC will rapidly increase calculation time.

    E : float (optional)
        Energy of MDCs to extract. Defaults to 0.

    dE : float (optional)
        MDC Integration range (represents the total range, i.e. integrates over +/- dE/2). Defaults to 0.

    k : float (optional)
        k or theta_par value of EDCs to extract. Defaults to 0.

    dk : float (optional)
        EDC integration range (represents the total range, i.e. integrates over +/- dk/2). Defaults to 0.

    scale : Boolean (optional)
        Whether to apply standard scaling to data (i.e. center all values in each dimension around zero with unit
        standard deviation). Defaults to False.

    norm : Boolean (optional)
        Whether to normalise the data at each spatial position. Defaults to False.

    Examples
    ------------
    from peaks import *

    SM = load(&#39;SM.ibw&#39;)

    SM.clusters_explore()  # Perform an exploratory k-means clustering analysis for numbers of clusters ranging from
                            1 to 6, applying a principal component analysis to the spatial mapping data using 3
                            principal components

    SM.clusters_explore(cluster_range=range(1,11), use_PCA=False, extract=&#39;MDC&#39;, E=73.42, dE=0.02)  # Perform an
                            exploratory k-means clustering analysis on spatial mapping MDCs for numbers of clusters
                            ranging from 1 to 10

    &#34;&#34;&#34;

    # Prevent unwanted overwriting of original data
    data = data.copy(deep=True)

    # Represent spatial mapping data as a tabular pandas dataframe
    df = data.xarray_to_ML_format(extract=extract, E=E, dE=dE, k=k, dk=dk, scale=scale, norm=norm)

    # Perform principal component analysis if PCA is True
    if use_PCA:
        pca = PCA(n_components=PCs)  # Define PCA model
        principal_components = pca.fit_transform(df)  # Fit PCA model to data and get principal components
        df = pd.DataFrame(data=principal_components)  # Get principal components in tabular pandas dataframe format

    # Perform k-means clustering analysis for the range of number of clusters (k) requested
    k_titles = [&#39;k=&#39; + str(k) for k in cluster_range]  # Titles for plots
    inertias = []  # Empty list to store model inertias (a metric that defines spread of a cluster)
    classification_maps = []  # Empty list to store classification maps (spatial maps of cluster labels)
    for num_clusters in tqdm(cluster_range, desc=&#39;Calculating&#39;, colour=&#39;CYAN&#39;):
        model, labels = perform_k_means(data=df, k=num_clusters, n_init=n_init)  # Perform k-means clustering
        inertias.append(model.inertia_)
        classification_map_data = labels.reshape(len(data.x1), len(data.x2))  # Reshape 1D labels to 2D
        classification_map = xr.DataArray(classification_map_data, dims=(&#34;x1&#34;, &#34;x2&#34;),
                                          coords={&#34;x1&#34;: data.x1, &#34;x2&#34;: data.x2})
        classification_maps.append(classification_map)

    inertias_xarray = xr.DataArray(inertias, dims=&#34;num_clusters&#34;,
                                   coords={&#34;num_clusters&#34;: cluster_range})  # Create xarray of inertias

    # Find optimal number of clusters
    for item in abs(inertias_xarray.differentiate(&#39;num_clusters&#39;)).norm():
        # If rate of decrease in inertia is below 20% of the initial decrease, found optimal number of clusters
        if float(item) &lt; 0.2:
            recommended_num_clusters = int(item.num_clusters)
            break
        # Else continue through loop
        else:
            recommended_num_clusters = int(item.num_clusters)

    # Remove progress bar
    clear_output()

    # Plot model inertia against number of clusters, and indicate recommended number of clusters
    inertias_xarray.plot(marker=&#39;o&#39;, figsize=(15, 4))
    plt.xlabel(&#39;Number of clusters (k)&#39;)
    plt.ylabel(&#39;Inertia&#39;)
    plt.axvline(recommended_num_clusters, c=&#39;black&#39;, linestyle=&#39;--&#39;)
    plt.title(r&#39;Optimal k$\approx$&#39; + str(recommended_num_clusters))
    plt.show()

    # Plot classification map dependence on number of clusters
    plot_grid(classification_maps, ncols=3, titles=k_titles, cmap=&#39;cividis&#39;, y=&#39;x2&#39;)
    plt.show()</code></pre>
</details>
</dd>
<dt id="peaks.ML.clustering.perform_k_means"><code class="name flex">
<span>def <span class="ident">perform_k_means</span></span>(<span>data, k=3, n_init='auto')</span>
</code></dt>
<dd>
<div class="desc"><p>Perform k-means clustering using scikit-learn.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>The data represented as a tabular pandas dataframe to perform clustering analysis on.</dd>
<dt><strong><code>k</code></strong> :&ensp;<code>int (optional)</code></dt>
<dd>Number of clusters. Defaults to 3.</dd>
<dt><strong><code>n_init</code></strong> :&ensp;<code>int, string (optional)</code></dt>
<dd>Number of times the k-means algorithm will be run with different centroid seeds. The final results will be the
best output of n_init consecutive runs in terms of inertia. Required since the kmeans algorithm can fall into
local minima, so repeats are required to check for this. Defaults to 'auto' (note: if an outdated scikit-learn
package is installed where 'auto' is not yet implemented, an error will arise. In this case set n_init=10, or
similar).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>sklearn.cluster._kmeans.KMeans</code></dt>
<dd>K-means clustering analysis model information.</dd>
<dt><strong><code>labels</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Array of spatially-dependent cluster assignments.</dd>
</dl>
<h2 id="examples">Examples</h2>
<p>from peaks import *</p>
<p>SM = load('SM.ibw')</p>
<p>SM_df = SM.xarray_to_ML_format()
# Get spatial mapping data in a tabular pandas dataframe format</p>
<p>model1, labels1 = perform_k_means(data=SM_df)
# Perform k-means clustering analysis for 3 clusters</p>
<p>model2, labels2 = perform_k_means(data=SM_df, k=4)
# Perform k-means clustering analysis for 4 clusters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def perform_k_means(data, k=3, n_init=&#34;auto&#34;):
    &#34;&#34;&#34;Perform k-means clustering using scikit-learn.

    Parameters
    ------------
    data : pd.DataFrame
        The data represented as a tabular pandas dataframe to perform clustering analysis on.

    k : int (optional)
        Number of clusters. Defaults to 3.

    n_init : int, string (optional)
        Number of times the k-means algorithm will be run with different centroid seeds. The final results will be the
        best output of n_init consecutive runs in terms of inertia. Required since the kmeans algorithm can fall into
        local minima, so repeats are required to check for this. Defaults to &#39;auto&#39; (note: if an outdated scikit-learn
        package is installed where &#39;auto&#39; is not yet implemented, an error will arise. In this case set n_init=10, or
        similar).

    Returns
    ------------
    model : sklearn.cluster._kmeans.KMeans
        K-means clustering analysis model information.

    labels : np.ndarray
        Array of spatially-dependent cluster assignments.

    Examples
    ------------
    from peaks import *

    SM = load(&#39;SM.ibw&#39;)

    SM_df = SM.xarray_to_ML_format()  # Get spatial mapping data in a tabular pandas dataframe format

    model1, labels1 = perform_k_means(data=SM_df)  # Perform k-means clustering analysis for 3 clusters

    model2, labels2 = perform_k_means(data=SM_df, k=4)  # Perform k-means clustering analysis for 4 clusters

    &#34;&#34;&#34;

    model = KMeans(n_clusters=k, n_init=n_init)  # Create a k-means clustering model
    model.fit(data)  # Fit the model to the sampling data
    labels = model.predict(data)  # Predict the labels of the samples, i.e. which cluster they belong to

    return model, labels</code></pre>
</details>
</dd>
<dt id="peaks.ML.clustering.xarray_to_ML_format"><code class="name flex">
<span>def <span class="ident">xarray_to_ML_format</span></span>(<span>data, extract='dispersion', E=0, dE=0, k=0, dk=0, scale=False, norm=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Represent spatial mapping data as a tabular pandas dataframe where each spatial position is a feature.
This dataframe format is used to represent data in machine learning functions.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>xr.DataArray</code></dt>
<dd>The spatial mapping data to change to a tabular pandas dataframe.</dd>
<dt><strong><code>extract</code></strong> :&ensp;<code>str (optional)</code></dt>
<dd>Determines what is extracted from spatial mapping data. Defaults to dispersion. Valid entries are:
dispersion
MDC
EDC
Selecting MDC/EDC will rapidly increase calculation time.</dd>
<dt><strong><code>E</code></strong> :&ensp;<code>float (optional)</code></dt>
<dd>Energy of MDCs to extract. Defaults to 0.</dd>
<dt><strong><code>dE</code></strong> :&ensp;<code>float (optional)</code></dt>
<dd>MDC Integration range (represents the total range, i.e. integrates over +/- dE/2). Defaults to 0.</dd>
<dt><strong><code>k</code></strong> :&ensp;<code>float (optional)</code></dt>
<dd>k or theta_par value of EDCs to extract. Defaults to 0.</dd>
<dt><strong><code>dk</code></strong> :&ensp;<code>float (optional)</code></dt>
<dd>EDC integration range (represents the total range, i.e. integrates over +/- dk/2). Defaults to 0.</dd>
<dt><strong><code>scale</code></strong> :&ensp;<code>Boolean (optional)</code></dt>
<dd>Whether to apply standard scaling to data (i.e. center all values in each dimension around zero with unit
standard deviation). Defaults to False.</dd>
<dt><strong><code>norm</code></strong> :&ensp;<code>Boolean (optional)</code></dt>
<dd>Whether to normalise the data at each spatial position. Defaults to False.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>The spatial mapping data represented as a tabular pandas dataframe.</dd>
</dl>
<h2 id="examples">Examples</h2>
<p>from peaks import *</p>
<p>SM = load('SM.ibw')</p>
<p>SM_df1 = SM.xarray_to_ML_format()
# Get spatial mapping data in a tabular pandas dataframe format</p>
<p>SM_df2 = SM.xarray_to_ML_format(extract='MDC', E=73.42, dE=0.02)
# Get spatial mapping MDCs in a tabular pandas
dataframe format</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@add_methods(xr.DataArray)
def xarray_to_ML_format(data, extract=&#39;dispersion&#39;, E=0, dE=0, k=0, dk=0, scale=False, norm=False):
    &#34;&#34;&#34;Represent spatial mapping data as a tabular pandas dataframe where each spatial position is a feature.
    This dataframe format is used to represent data in machine learning functions.

    Parameters
    ------------
    data : xr.DataArray
        The spatial mapping data to change to a tabular pandas dataframe.

    extract : str (optional)
        Determines what is extracted from spatial mapping data. Defaults to dispersion. Valid entries are:
            dispersion
            MDC
            EDC
        Selecting MDC/EDC will rapidly increase calculation time.

    E : float (optional)
        Energy of MDCs to extract. Defaults to 0.

    dE : float (optional)
        MDC Integration range (represents the total range, i.e. integrates over +/- dE/2). Defaults to 0.

    k : float (optional)
        k or theta_par value of EDCs to extract. Defaults to 0.

    dk : float (optional)
        EDC integration range (represents the total range, i.e. integrates over +/- dk/2). Defaults to 0.

    scale : Boolean (optional)
        Whether to apply standard scaling to data (i.e. center all values in each dimension around zero with unit
        standard deviation). Defaults to False.

    norm : Boolean (optional)
        Whether to normalise the data at each spatial position. Defaults to False.

    Returns
    ------------
    df : pd.DataFrame
        The spatial mapping data represented as a tabular pandas dataframe.

    Examples
    ------------
    from peaks import *

    SM = load(&#39;SM.ibw&#39;)

    SM_df1 = SM.xarray_to_ML_format()  # Get spatial mapping data in a tabular pandas dataframe format

    SM_df2 = SM.xarray_to_ML_format(extract=&#39;MDC&#39;, E=73.42, dE=0.02)  # Get spatial mapping MDCs in a tabular pandas
                                                                        dataframe format

    &#34;&#34;&#34;

    # List to store spatial mapping data
    sample_data = []

    # Loop through spatial positions
    x1_vals = data.x1.data
    x2_vals = data.x2.data
    for i in range(len(x1_vals)):
        for j in range(len(x2_vals)):
            current_disp = data.isel(x1=i).isel(x2=j)

            # Apply normalisation at each spatial position if requested
            if norm:
                current_disp = current_disp.norm()

            # If we want to represent each feature to be a full dispersion
            if extract == &#39;dispersion&#39;:
                # Get total number of dimensions (pixels) of dispersion
                dimensions = 1
                for coord_dim in current_disp.shape:
                    dimensions = dimensions * coord_dim

                # Reshape current dispersion into tabular form and append row to data
                reshaped_disp = current_disp.data.reshape(dimensions)
                sample_data.append(reshaped_disp)

            # If we want to represent each feature to be an MDC
            elif extract == &#39;MDC&#39;:
                # Extract MDC from dispersion
                current_MDC = current_disp.MDC(E=E, dE=dE)
                sample_data.append(current_MDC.data)

            # If we want to represent each feature to be an EDC
            elif extract == &#39;EDC&#39;:
                # Extract EDC from dispersion
                current_EDC = current_disp.EDC(k=k, dk=dk)
                sample_data.append(current_EDC.data)

            # Else user has entered an invalid method argument
            else:
                raise Exception(&#39;Method must be dispersion, MDC or EDC&#39;)

    # Create tabular pandas dataframe
    df = pd.DataFrame(data=np.array(sample_data))

    if scale:
        # Apply optional standard scaling (center all values in each dimension around zero with unit standard deviation)
        std_scaler = StandardScaler()
        df = pd.DataFrame(data=std_scaler.fit_transform(df))

    return df</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="peaks.ML" href="index.html">peaks.ML</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="peaks.ML.clustering.PCA_explore" href="#peaks.ML.clustering.PCA_explore">PCA_explore</a></code></li>
<li><code><a title="peaks.ML.clustering.clusters" href="#peaks.ML.clustering.clusters">clusters</a></code></li>
<li><code><a title="peaks.ML.clustering.clusters_explore" href="#peaks.ML.clustering.clusters_explore">clusters_explore</a></code></li>
<li><code><a title="peaks.ML.clustering.perform_k_means" href="#peaks.ML.clustering.perform_k_means">perform_k_means</a></code></li>
<li><code><a title="peaks.ML.clustering.xarray_to_ML_format" href="#peaks.ML.clustering.xarray_to_ML_format">xarray_to_ML_format</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>